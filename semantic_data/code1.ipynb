{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d2e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "def load_smpl_data(person_id, npz_root, walk_df):\n",
    "    features = []\n",
    "    \n",
    "    # Filter the walks for this person\n",
    "    person_walks = walk_df[walk_df['ID'] == person_id]\n",
    "\n",
    "    for _, row in person_walks.iterrows():\n",
    "        file_path = os.path.join(npz_root, row['file_path'])\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        try:\n",
    "            data = np.load(file_path)\n",
    "            frame_feats = []\n",
    "\n",
    "            if 'pose' in data:\n",
    "                frame_feats.append(data['pose'].mean(axis=0) if data['pose'].ndim > 1 else data['pose'])  # (72,)\n",
    "            if 'shape' in data:\n",
    "                frame_feats.append(data['shape'].mean(axis=0) if data['shape'].ndim > 1 else data['shape'])  # (10,)\n",
    "            if 'global_t' in data:\n",
    "                frame_feats.append(data['global_t'].mean(axis=0))  # (3,)\n",
    "            if 'focal_l' in data:\n",
    "                frame_feats.append([np.mean(data['focal_l'])])  # scalar\n",
    "            if 'pred_joints' in data:\n",
    "                frame_feats.append(data['pred_joints'].mean(axis=(0, 1)))  # (3,)\n",
    "\n",
    "            # Optionally add metadata like viewpoint, variation\n",
    "            frame_feats.append([row['viewpoint']])\n",
    "            frame_feats.append([hash(row['variation']) % 1000])  # or use OneHotEncoding if needed\n",
    "\n",
    "            features.append(np.concatenate(frame_feats))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(features) == 0:\n",
    "        raise ValueError(f\"No valid files found for ID {person_id}\")\n",
    "\n",
    "    return np.mean(features, axis=0)  # aggregate over all variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc69924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load walk metadata from CSV\n",
    "def load_walk_metadata(walk_csv):\n",
    "    walk_df = pd.read_csv(walk_csv)\n",
    "    walk_df['file_path'] = walk_df.apply(lambda row: f\"{row['ID']}/{row['file_id'].replace(':', '_')}.npz\", axis=1)\n",
    "    return walk_df\n",
    "\n",
    "# Load GHQ labels from CSV\n",
    "def load_ghq_labels(csv_file):\n",
    "    ghq_data = pd.read_csv(csv_file)\n",
    "    return ghq_data.set_index('ID')['GHQ_Label'].to_dict()  # Create a mapping from person_id to ghq_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061b51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(npz_root, ghq_csv, walk_csv):\n",
    "    walk_df = load_walk_metadata(walk_csv)\n",
    "    ghq_labels = load_ghq_labels(ghq_csv)\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "\n",
    "    for person_id in ghq_labels.keys():\n",
    "        try:\n",
    "            feats = load_smpl_data(person_id, npz_root, walk_df)\n",
    "            all_features.append(feats)\n",
    "            all_labels.append(ghq_labels[person_id])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping ID {person_id}: {e}\")\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee6c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "walk_df = 'walks.csv'\n",
    "npz_folder = 'smpl'  # Replace with the path to your npz folder\n",
    "ghq_csv = 'subset.csv'  # Replace with the path to your GHQ labels CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a00c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = process_data(npz_root=\"smpl\", ghq_csv=\"subset.csv\", walk_csv=\"walks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b12fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming labels are like ['Major Distress', 'Typical', ...]\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Now use `labels_encoded` in model training instead of `labels`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa546b1",
   "metadata": {},
   "source": [
    "RANDOM forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2125dc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Major Distress       0.00      0.00      0.00        11\n",
      "Minor Distress       0.83      0.25      0.38        20\n",
      "       Typical       0.72      1.00      0.83        63\n",
      "\n",
      "      accuracy                           0.72        94\n",
      "     macro avg       0.52      0.42      0.41        94\n",
      "  weighted avg       0.66      0.72      0.64        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(npz_folder, ghq_csv,walk_df):\n",
    "    # Process the data\n",
    "    features, labels = process_data(npz_folder, ghq_csv,walk_df)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.3, random_state=280, stratify=labels)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "\n",
    "train_and_evaluate(npz_folder, ghq_csv, walk_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5e478",
   "metadata": {},
   "source": [
    "Random is\n",
    "Strengths: The model does well at identifying \"Typical\" distress, making it effective for the more frequent class.\n",
    "Weaknesses: The model fails to identify \"Major Distress\" and has significant trouble with \"Minor Distress.\" These issues are likely due to class imbalance, insufficient training data for certain classes, and/or overfitting on the dominant class (\"Typical\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db266f4",
   "metadata": {},
   "source": [
    "basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a13c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 1. Neural Network Model (Example)\n",
    "# Build the neural network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))  # First layer with input shape\n",
    "    model.add(Dense(32, activation='relu'))  # Hidden layer\n",
    "    model.add(Dense(3, activation='softmax'))  # Output layer (assuming 3 classes in the target)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2466821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\semantic_data\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4854 - loss: 1.0386 - val_accuracy: 0.6984 - val_loss: 0.7821\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6554 - loss: 0.7931 - val_accuracy: 0.7619 - val_loss: 0.7357\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6652 - loss: 0.7705 - val_accuracy: 0.7302 - val_loss: 0.7323\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6649 - loss: 0.7425 - val_accuracy: 0.7302 - val_loss: 0.7208\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6966 - loss: 0.7047 - val_accuracy: 0.7143 - val_loss: 0.6982\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7005 - loss: 0.6546 - val_accuracy: 0.7302 - val_loss: 0.6922\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7699 - loss: 0.5946 - val_accuracy: 0.7460 - val_loss: 0.6871\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7365 - loss: 0.6256 - val_accuracy: 0.6984 - val_loss: 0.6884\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7776 - loss: 0.5539 - val_accuracy: 0.7143 - val_loss: 0.6838\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7799 - loss: 0.5587 - val_accuracy: 0.7143 - val_loss: 0.6890\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.43      0.27      0.33        11\n",
      "           2       0.76      0.89      0.82        47\n",
      "\n",
      "    accuracy                           0.71        63\n",
      "   macro avg       0.40      0.39      0.39        63\n",
      "weighted avg       0.64      0.71      0.67        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_evaluate_model2(features, labels):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=280)\n",
    "    \n",
    "    # Normalize the features (important for neural networks)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Build and train the neural network\n",
    "    model = build_nn_model(X_train.shape[1])  # Input dimension based on features\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "\n",
    "# Train and evaluate using the processed data\n",
    "\n",
    "train_evaluate_model2(features, labels_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5193c",
   "metadata": {},
   "source": [
    " 1. Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53b95560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       1.00      0.18      0.31        11\n",
      "           2       0.77      0.98      0.86        47\n",
      "\n",
      "    accuracy                           0.76        63\n",
      "   macro avg       0.59      0.39      0.39        63\n",
      "weighted avg       0.75      0.76      0.70        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def try_xgboost(features, labels):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=280)\n",
    "\n",
    "    model = XGBClassifier( eval_metric='mlogloss')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "\n",
    "model, scaler = try_xgboost(features, labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14397119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_smpl_features(npz_path, viewpoint=0, variation_hash=0):\n",
    "    \"\"\"\n",
    "    Load a single .npz and turn it into the same feature‐vector you used for training.\n",
    "    If you don’t have viewpoint/variation info, you can default them (e.g. 0).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(f\"{npz_path} not found\")\n",
    "\n",
    "    data = np.load(npz_path)\n",
    "    feats = []\n",
    "\n",
    "    # pose: (72,)\n",
    "    if 'pose' in data:\n",
    "        p = data['pose']\n",
    "        feats.append(p.mean(axis=0) if p.ndim > 1 else p)\n",
    "\n",
    "    # shape: (10,)\n",
    "    if 'shape' in data:\n",
    "        s = data['shape']\n",
    "        feats.append(s.mean(axis=0) if s.ndim > 1 else s)\n",
    "\n",
    "    # global translation: (3,)\n",
    "    if 'global_t' in data:\n",
    "        feats.append(data['global_t'].mean(axis=0))\n",
    "\n",
    "    # focal length (scalar)\n",
    "    if 'focal_l' in data:\n",
    "        feats.append([np.mean(data['focal_l'])])\n",
    "\n",
    "    # predicted joints: flatten mean over frames & joints\n",
    "    if 'pred_joints' in data:\n",
    "        # data['pred_joints'] shape e.g. (num_frames, num_joints, 3)\n",
    "        feats.append(data['pred_joints'].mean(axis=(0,1)))\n",
    "\n",
    "    # add—or default—your “metadata” slots:\n",
    "    feats.append([viewpoint])\n",
    "    feats.append([variation_hash])\n",
    "\n",
    "    # final 1D vector\n",
    "    return np.concatenate(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a026eeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# after `model.fit(...)` and `le.fit(...)` in your training code:\n",
    "dump(scaler, 'scaler.joblib')\n",
    "dump(le,       'label_encoder.joblib')\n",
    "dump(model,    'xgb_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b061de6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_feature_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myour_feature_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_smpl_features  \u001b[38;5;66;03m# wherever you put it\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_from_npz\u001b[39m(npz_path, viewpoint=\u001b[32m0\u001b[39m, variation_hash=\u001b[32m0\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# 1. extract\u001b[39;00m\n\u001b[32m      8\u001b[39m     feats = extract_smpl_features(npz_path, viewpoint, variation_hash).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'your_feature_module'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48005c44",
   "metadata": {},
   "source": [
    "2. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe6f7ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.75      1.00      0.85        47\n",
      "\n",
      "    accuracy                           0.75        63\n",
      "   macro avg       0.25      0.33      0.28        63\n",
      "weighted avg       0.56      0.75      0.64        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def try_svm(features, labels):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=280)\n",
    "\n",
    "    model = SVC(kernel='rbf', probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "try_svm(features, labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb59d0",
   "metadata": {},
   "source": [
    "3. Logistic Regression (Multiclass Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdf16976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.20      0.09      0.12        11\n",
      "           2       0.74      0.91      0.82        47\n",
      "\n",
      "    accuracy                           0.70        63\n",
      "   macro avg       0.31      0.34      0.31        63\n",
      "weighted avg       0.59      0.70      0.63        63\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\semantic_data\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def try_logistic_regression(features, labels):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=280)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Logistic Regression Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "try_logistic_regression(features, labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fe8d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Major Distress       0.00      0.00      0.00         5\n",
      "Minor Distress       0.00      0.00      0.00        11\n",
      "       Typical       0.75      1.00      0.85        47\n",
      "\n",
      "      accuracy                           0.75        63\n",
      "     macro avg       0.25      0.33      0.28        63\n",
      "  weighted avg       0.56      0.75      0.64        63\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\semantic_data\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\New folder\\semantic_data\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\New folder\\semantic_data\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def train_ensemble_model(features, labels):\n",
    "    # Encode string labels to integers\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(labels)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=280)\n",
    "\n",
    "    # Define base models\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    svc = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "    # Combine them using a voting classifier (soft voting is better for probabilities)\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('rf', rf),\n",
    "        ('svc', svc)\n",
    "    ], voting='soft')  # Use 'hard' for majority voting\n",
    "\n",
    "    # Train ensemble\n",
    "    ensemble.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = ensemble.predict(X_test)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "train_ensemble_model(features, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
